{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/SVVnnHMjst/Qrz/UhfH2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HardikPrabhu/Quick-tutorials-for-pytorch/blob/main/Convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "z8WZwsdWgiyO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convolutions\n",
        "\n",
        "For continuous one dimensional functions f and g, their convolution (f * g) is defined as:\n",
        "$$(f * g)(t) = ∫ f(τ)g(t - τ) dτ$$\n"
      ],
      "metadata": {
        "id": "EA5Aniz40-PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Think of convolution between two real valued functions as taking one function and flipping it on the x axis and adding a small step (t) to it and taling the weighted sum across all the points in the domain to get the output as function of the step.\n",
        "\n"
      ],
      "metadata": {
        "id": "JPM1dYnJ1WHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discrete Convolution\n",
        "* For discrete functions, convolution is defined as:\n",
        "(f ∗ g)[n] = Σ f[m]g[n - m]\n",
        "\n",
        "This involves flipping one of the signals (the g[n - m] term represents g flipped and shifted).\n",
        "\n",
        "Cross-Correlation (What We Actually Use)\n",
        "* In practice, we work with cross-correlation instead, which is defined as:\n",
        "(f * g)[n] = Σ f[n + m]g[m]\n",
        "\n",
        "\n",
        "Key differences:\n",
        "\n",
        "No flipping: Cross-correlation doesn't flip the second function\n",
        "More intuitive: Direct element-wise multiplication and summation\n",
        "\n"
      ],
      "metadata": {
        "id": "dpvB5lfBE_6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution is a Linear Trasformation\n",
        "\n",
        "If we consider the input vector's ith dimension as a output of a function:\n",
        " $f(i) =x_i$\n",
        "\n",
        "and $g(i) = w_i$, ith weight in the kernel (set of parameters).\n",
        "\n",
        "\n",
        "Then Convolution is at the end of the day a linear trasformation, its just the number of parameters reduce by using the same set of parameters (kernel) over and over again to create a sparse (with lots of zeros) Matrix for the linear transforamtion done.\n",
        "\n",
        "So, can directly be used in between layers just as we use regular weight matrices.\n",
        "\n",
        "Now its a two step process:\n",
        " 1. Create appropriate weight matrix using the kernel\n",
        " 2. Then do the linear transfomration (this is what is meant by parellel processing)\n",
        "\n",
        "\n",
        " Example:\n",
        "Consider the input vector x  as the following:\n",
        "\n",
        "$x = [x1,x2,x3,x4]^T$\n",
        "\n",
        "and output $y = [y1,y2]^T$\n",
        "\n",
        "In normal linear transformation we would require:\n",
        "\n",
        "$y =Wx$, we requre 2x4 matrix (8 params), and the connection is dense ($y_i$ depends on all the $x_j$'s)\n",
        "\n",
        "## Convolution = sparse connection + parameter sharing\n",
        "\n",
        "## Sparse Conection:\n",
        "\n",
        "\n",
        "\n",
        "$y_1$ depends on $(x_1, x_2, x_3)$\n",
        "$y_2$ depends on $(x_2, x_3, x_4)$\n",
        "\n",
        "\n",
        "Weight matrix becomes:\n",
        "$W$ = \\begin{bmatrix}\n",
        "w_{11} & w_{12} & w_{13} & 0 \\end{bmatrix}\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & w_{22} & w_{23} & w_{24}\n",
        "\\end{bmatrix}\n",
        "\n",
        "## Parameter sharing:\n",
        "\n",
        "\n",
        "For true convolution (parameter sharing), we would use the same kernel weights:\n",
        "$W$ = \\begin{bmatrix}\n",
        "w_1 & w_2 & w_3 & 0 \\\n",
        "\\end{bmatrix}\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & w_1 & w_2 & w_3\n",
        "\\end{bmatrix}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0QtwhwKj4eJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Convolution Rules\n",
        "\n",
        "##Output Size Determination\n",
        "\n",
        "When using the default convolution layer in PyTorch:\n",
        "Output dimension formula:\n",
        "\n",
        "$\\text{out_dim} = \\text{input_dim} - \\text{kernel_size} + 1$\n",
        "\n",
        "##Mathematical Foundation\n",
        "The convolution operation is defined as:\n",
        "\n",
        "$(f * g)[n] = \\sum_{m} f[n + m] \\cdot g[m]$\n",
        "\n",
        "This works as long as $f(n+m)$ is defined within the valid input range. The output size naturally follows from the valid positions where the kernel can be applied without going out of bounds."
      ],
      "metadata": {
        "id": "i8HLeYIDE2N_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "fzwWLStyWtj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-D Convolution using pytorch\n"
      ],
      "metadata": {
        "id": "t-CrVAg5-Fnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we are ready to create our first convolution layer\n",
        "size = 3\n",
        "input_dim = 10\n",
        "kernel = torch.rand(size).view(1,-1)\n",
        "x = torch.arange(1,input_dim+1,dtype=torch.float)\n",
        "\n",
        "# W is going to be (8,10)\n",
        "W = torch.zeros(8,10)\n",
        "for i in range(8):\n",
        "  W[i,i:i+size] = kernel\n",
        "print(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjd3ZkmKAkn0",
        "outputId": "9b39bb77-a918-4f45-8663-debf47984ef5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3447, 0.5001, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.3447, 0.5001, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.3447, 0.5001, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.3447, 0.5001, 0.2005, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3447, 0.5001, 0.2005, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3447, 0.5001, 0.2005, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3447, 0.5001, 0.2005,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3447, 0.5001,\n",
            "         0.2005]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finally we get\n",
        "y = W@x\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOC8i8dd5Ivg",
        "outputId": "15cd1762-d058-4b00-9fe8-f22526abd820"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.9466, 2.9920, 4.0374, 5.0828, 6.1282, 7.1736, 8.2190, 9.2643])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember Neural nets support sequence of such inputs:\n",
        "$Y = XW^T$"
      ],
      "metadata": {
        "id": "lnFxmmvCIAEY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eGDQ3bbC09TQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class Conv1D(torch.nn.Module):\n",
        "   def __init__(self, kernel_size, input_dim):\n",
        "       super().__init__()\n",
        "       self.kernel_size = kernel_size\n",
        "       self.input_dim = input_dim\n",
        "       self.output_dim = input_dim - kernel_size + 1\n",
        "\n",
        "       # Initialize kernel weights\n",
        "       self.kernel = torch.nn.Parameter(torch.randn(kernel_size)) # Optimizer needs to see this, ony using requires_grad=True, grad is calcualted but the optimizer never updates it!\n",
        "\n",
        "   def forward(self, x):\n",
        "       # Create sparse weight matrix W of shape (output_dim, input_dim)\n",
        "       W = torch.zeros(self.output_dim, self.input_dim, device=x.device)\n",
        "       for i in range(self.output_dim):\n",
        "           W[i, i:i+self.kernel_size] = self.kernel\n",
        "\n",
        "       # Apply convolution: Y = X @ W^T\n",
        "       return x @ W.T\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example\n",
        "batch_size = 4\n",
        "input_dim = 10\n",
        "kernel_size = 3\n",
        "\n",
        "conv_layer = Conv1D(kernel_size, input_dim)\n",
        "x = torch.randn(batch_size, input_dim)\n",
        "y = conv_layer(x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")   # (4, 10)\n",
        "print(f\"Output shape: {y.shape}\")  # (4, 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e43576AAPtP2",
        "outputId": "e3e3efdf-18ca-4b36-88d4-7fe034568d3e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 10])\n",
            "Output shape: torch.Size([4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stride and Dilation\n",
        "\n",
        "## Stride\n",
        "Controls how much you move the kernel between applications:\n",
        "\n",
        "Stride = 1 (default): move kernel 1 position at a time\n",
        "```\n",
        "# Input: [1,2,3,4,5,6,7,8,9,10], kernel_size=3\n",
        "# Positions: [1,2,3], [2,3,4], [3,4,5], [4,5,6], [5,6,7], [6,7,8], [7,8,9], [8,9,10]\n",
        "\n",
        "\n",
        "W = [[w1, w2, w3, 0,  0,  0,  0,  0,  0,  0 ],\n",
        "     [0,  w1, w2, w3, 0,  0,  0,  0,  0,  0 ],\n",
        "     [0,  0,  w1, w2, w3, 0,  0,  0,  0,  0 ],\n",
        "     [0,  0,  0,  w1, w2, w3, 0,  0,  0,  0 ],\n",
        "     [0,  0,  0,  0,  w1, w2, w3, 0,  0,  0 ],\n",
        "     [0,  0,  0,  0,  0,  w1, w2, w3, 0,  0 ],\n",
        "     [0,  0,  0,  0,  0,  0,  w1, w2, w3, 0 ],\n",
        "     [0,  0,  0,  0,  0,  0,  0,  w1, w2, w3]]\n",
        "\n",
        "\n",
        "```\n",
        "Stride = 2: move kernel 2 positions at a time  \n",
        "\n",
        "```\n",
        "# Positions: [1,2,3], [3,4,5], [5,6,7], [7,8,9]\n",
        "# Output size = (input_dim - kernel_size) // stride + 1\n",
        "\n",
        "\n",
        "W = [[w1, w2, w3, 0,  0,  0,  0,  0,  0,  0 ],\n",
        "     [0,  0,  w1, w2, w3, 0,  0,  0,  0,  0 ],\n",
        "     [0,  0,  0,  0,  w1, w2, w3, 0,  0,  0 ],\n",
        "     [0,  0,  0,  0,  0,  0,  w1, w2, w3, 0 ]]\n",
        "\n",
        "```\n",
        "\n",
        "## Dilation\n",
        "Controls the spacing within the kernel itself:\n",
        "\n",
        "\n",
        "Dilation = 1 (default): kernel elements are adjacent\n",
        "\n",
        "Dilation = 2: skip 1 element between kernel positions\n",
        "\n",
        "```\n",
        "# Positions: [1,3,5], [2,4,6], [3,5,7], [4,6,8], etc.\n",
        "# Output size = input_dim - (kernel_size-1) * dilation = 10 - (3-1)*2 = 6\n",
        "\n",
        "W = [[w1, 0,  w2, 0,  w3, 0,  0,  0,  0,  0 ],\n",
        "     [0,  w1, 0,  w2, 0,  w3, 0,  0,  0,  0 ],\n",
        "     [0,  0,  w1, 0,  w2, 0,  w3, 0,  0,  0 ],\n",
        "     [0,  0,  0,  w1, 0,  w2, 0,  w3, 0,  0 ],\n",
        "     [0,  0,  0,  0,  w1, 0,  w2, 0,  w3, 0 ],\n",
        "     [0,  0,  0,  0,  0,  w1, 0,  w2, 0,  w3]]\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "SCpxeJfnMFsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note here is a lazy example but could use different indexing techniques shown in the first notebook to store values form kerenel into weight matrix:"
      ],
      "metadata": {
        "id": "zY5AGgkeQLHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For visualisation on 2-D Conv for the same concepts refer: https://github.com/vdumoulin/conv_arithmetic/tree/master"
      ],
      "metadata": {
        "id": "ClVx_YJ2XEXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation: First do the dialtion, then do the stride\n",
        "\n",
        "Think of dialtion as incresing kernel size but adding zeros in between"
      ],
      "metadata": {
        "id": "YUG9rbCmSS9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1D(torch.nn.Module):\n",
        "   def __init__(self, kernel_size, input_dim, stride=1, dilation=1):\n",
        "       super().__init__()\n",
        "       self.kernel_size = kernel_size\n",
        "       self.input_dim = input_dim\n",
        "       self.stride = stride\n",
        "       self.dilation = dilation\n",
        "\n",
        "       # Calculate output dimension\n",
        "       effective_kernel_size = (kernel_size - 1) * dilation + 1\n",
        "       self.output_dim = (input_dim - effective_kernel_size) // stride + 1\n",
        "\n",
        "       # Initialize kernel weights\n",
        "       self.kernel = torch.nn.Parameter(torch.randn(kernel_size))\n",
        "\n",
        "   def forward(self, x):\n",
        "       # x shape: (batch_size, input_dim)\n",
        "       # Create sparse weight matrix W\n",
        "       W = torch.zeros(self.output_dim, self.input_dim, device=x.device)\n",
        "\n",
        "       for i in range(self.output_dim):\n",
        "           start_pos = i * self.stride\n",
        "           for j in range(self.kernel_size):\n",
        "               col_idx = start_pos + j * self.dilation\n",
        "               if col_idx < self.input_dim:\n",
        "                   W[i, col_idx] = self.kernel[j]\n",
        "\n",
        "\n",
        "       return x @ W.T\n"
      ],
      "metadata": {
        "id": "erG1TyL_JG4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding\n",
        "\n",
        "\n",
        "If you want to return the output in the same dimension as of input, we need to pad it with zeros first so that\n",
        "\n",
        "output_dim = f(input_dim + padding, stride,dilation) is same as input_dim\n",
        "\n",
        "## Note: The math for how many zeros we should pad may not add up when we have stride and dialaton (Read specific implementations to see how they work)"
      ],
      "metadata": {
        "id": "DyePN-icQ0d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Symmetric padding (default)\n",
        "x = torch.tensor([[1., 2., 3., 4., 5.]])\n",
        "padded_sym = F.pad(x, (2, 2), value=0)  # (left_pad, right_pad)\n",
        "print(f\"Symmetric:  {padded_sym}\")  # [0, 0, 1, 2, 3, 4, 5, 0, 0]\n",
        "\n",
        "# Asymmetric padding\n",
        "padded_asym = F.pad(x, (1, 3), value=0)  # 1 zero left, 3 zeros right\n",
        "print(f\"Asymmetric: {padded_asym}\")     # [0, 1, 2, 3, 4, 5, 0, 0, 0]\n",
        "\n",
        "# Only left padding\n",
        "padded_left = F.pad(x, (2, 0), value=0)\n",
        "print(f\"Left only:  {padded_left}\")     # [0, 0, 1, 2, 3, 4, 5]\n",
        "\n",
        "# Only right padding\n",
        "padded_right = F.pad(x, (0, 2), value=0)\n",
        "print(f\"Right only: {padded_right}\")    # [1, 2, 3, 4, 5, 0, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeSu8ta4R5qH",
        "outputId": "e434af5c-608f-4dca-9849-cba3f17a60dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symmetric:  tensor([[0., 0., 1., 2., 3., 4., 5., 0., 0.]])\n",
            "Asymmetric: tensor([[0., 1., 2., 3., 4., 5., 0., 0., 0.]])\n",
            "Left only:  tensor([[0., 0., 1., 2., 3., 4., 5.]])\n",
            "Right only: tensor([[1., 2., 3., 4., 5., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Channels: Processing multiple signals at a time"
      ],
      "metadata": {
        "id": "a7vUNMbaVYPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## in_channels=2, out_channels=1\n",
        "\n",
        "Input: Two separate sequences:\n",
        "\n",
        "          Channel1: [a,b,c,d,e,f,g,h]\n",
        "          Channel2: [i,j,k,l,m,n,o,p]\n",
        "\n",
        "Filters:\n",
        "- Channel1 filter: [w1, w2, w3]\n",
        "- Channel2 filter: [w4, w5, w6]\n",
        "\n",
        "Operation: w1*a + w2*b + w3*c + w4*i + w5*j + w6*k = output[0]"
      ],
      "metadata": {
        "id": "pTnAvsHqXmkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only difference is we dont create two seperate outputs but actually sum the two to create one output."
      ],
      "metadata": {
        "id": "9zH8p9lteESO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Output Channels: in_channels=2, out_channels=3\n",
        "\n",
        "Each output channel is essentially running its own complete convolution across all input channels!\n",
        "\n",
        "Total kernels needed: 2 input × 3 output = 6 separate kernels"
      ],
      "metadata": {
        "id": "4WCsggwVe0nK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Output Channel 0:\n",
        "\n",
        "```\n",
        "\n",
        "├── Input Channel 0 kernel: [w1_00, w2_00, w3_00]\n",
        "└── Input Channel 1 kernel: [w4_00, w5_00, w6_00]\n",
        "```\n",
        "For Output Channel 1:\n",
        "```\n",
        "├── Input Channel 0 kernel: [w1_01, w2_01, w3_01]  \n",
        "└── Input Channel 1 kernel: [w4_01, w5_01, w6_01]\n",
        "```\n",
        "For Output Channel 2:\n",
        "```\n",
        "├── Input Channel 0 kernel: [w1_02, w2_02, w3_02]\n",
        "└── Input Channel 1 kernel: [w4_02, w5_02, w6_02]\n",
        "```"
      ],
      "metadata": {
        "id": "gaXlAYQRfS_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equivalence between 1-d conv with muliple channels to perfoming 2-d conv\n",
        "\n",
        "Note: 2-d conv across a grayscale(matrix) image can be viewwed as 1-conv across mutiple row chanels and output channel =1"
      ],
      "metadata": {
        "id": "tHYATH52fz8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample grayscale image (4x5)\n",
        "grayscale_image = torch.tensor([\n",
        "        [1.0, 2.0, 3.0, 4.0, 5.0],\n",
        "        [6.0, 7.0, 8.0, 9.0, 10.0],\n",
        "        [11.0, 12.0, 13.0, 14.0, 15.0],\n",
        "        [16.0, 17.0, 18.0, 19.0, 20.0]\n",
        "    ])"
      ],
      "metadata": {
        "id": "lJLyR7iLgq-A"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape for 2D conv: [batch, channels, height, width]\n",
        "img_2d = grayscale_image.unsqueeze(0).unsqueeze(0)  # [1, 1, 4, 5]\n",
        "print(f\"2D Conv Input Shape: {img_2d.shape}\")\n",
        "\n",
        "# Create 2D convolution layer\n",
        "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n",
        "\n",
        "# Set specific weights for demonstration\n",
        "weights_2d = torch.tensor([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.4, 0.5, 0.6],\n",
        "    [0.7, 0.8, 0.9]\n",
        "]).unsqueeze(0).unsqueeze(0)  # [1, 1, 3, 3]\n",
        "\n",
        "conv2d.weight.data = weights_2d\n",
        "print(\"2D Kernel:\")\n",
        "print(weights_2d.squeeze())\n",
        "\n",
        "# Apply 2D convolution\n",
        "output_2d = conv2d(img_2d)\n",
        "print(f\"2D Conv Output Shape: {output_2d.shape}\")\n",
        "print(\"2D Conv Output:\")\n",
        "print(output_2d.squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6etkz3Ng8Xh",
        "outputId": "d548aa22-a506-49a5-92f7-d7ffca69f7d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2D Conv Input Shape: torch.Size([1, 1, 4, 5])\n",
            "2D Kernel:\n",
            "tensor([[0.1000, 0.2000, 0.3000],\n",
            "        [0.4000, 0.5000, 0.6000],\n",
            "        [0.7000, 0.8000, 0.9000]])\n",
            "2D Conv Output Shape: torch.Size([1, 1, 2, 3])\n",
            "2D Conv Output:\n",
            "tensor([[41.1000, 45.6000, 50.1000],\n",
            "        [63.6000, 68.1000, 72.6000]], grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now viewing as 1-D Conv across row number of input channels and 2 output channel"
      ],
      "metadata": {
        "id": "b1vVJYWWjF1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create 1D convolution: 4 input channels (rows), 2 output channels\n",
        "conv1d = nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, bias=False)\n",
        "\n",
        "# Set weights to match 2D kernel\n",
        "# For 2 output channels, we need to simulate the 2D kernel sliding vertically\n",
        "weights_1d = torch.tensor([\n",
        "    # Output Channel 0: Uses rows 0,1,2 (top position of 3x3 kernel)\n",
        "    [[0.1, 0.2, 0.3],  # Kernel for input channel 0 (row 0)\n",
        "      [0.4, 0.5, 0.6],  # Kernel for input channel 1 (row 1)\n",
        "      [0.7, 0.8, 0.9],  # Kernel for input channel 2 (row 2)\n",
        "      [0.0, 0.0, 0.0]], # Kernel for input channel 3 (row 3) - not used\n",
        "\n",
        "    # Output Channel 1: Uses rows 1,2,3 (bottom position of 3x3 kernel)\n",
        "    [[0.0, 0.0, 0.0],  # Kernel for input channel 0 (row 0) - not used\n",
        "      [0.1, 0.2, 0.3],  # Kernel for input channel 1 (row 1)\n",
        "      [0.4, 0.5, 0.6],  # Kernel for input channel 2 (row 2)\n",
        "      [0.7, 0.8, 0.9]]  # Kernel for input channel 3 (row 3)\n",
        "])  # [2, 4, 3]\n",
        "\n",
        "conv1d.weight.data = weights_1d\n",
        "print(\"1D Kernels for 2 output channels:\")\n",
        "print(\"Output Channel 0 (simulates 3x3 kernel at top position):\")\n",
        "for i, kernel in enumerate(weights_1d[0]):\n",
        "    print(f\"  Input Channel {i} kernel: {kernel.tolist()}\")\n",
        "print(\"Output Channel 1 (simulates 3x3 kernel at bottom position):\")\n",
        "for i, kernel in enumerate(weights_1d[1]):\n",
        "    print(f\"  Input Channel {i} kernel: {kernel.tolist()}\")\n",
        "\n",
        "# Apply 1D convolution\n",
        "output_1d = conv1d(img_1d)\n",
        "print(f\"1D Conv Output Shape: {output_1d.shape}\")\n",
        "print(\"1D Conv Output (2 channels):\")\n",
        "print(output_1d.squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKMoJRbChp2o",
        "outputId": "7892f4b2-f6fa-403d-dc87-a9ebc133acce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1D Kernels for 2 output channels:\n",
            "Output Channel 0 (simulates 3x3 kernel at top position):\n",
            "  Input Channel 0 kernel: [0.10000000149011612, 0.20000000298023224, 0.30000001192092896]\n",
            "  Input Channel 1 kernel: [0.4000000059604645, 0.5, 0.6000000238418579]\n",
            "  Input Channel 2 kernel: [0.699999988079071, 0.800000011920929, 0.8999999761581421]\n",
            "  Input Channel 3 kernel: [0.0, 0.0, 0.0]\n",
            "Output Channel 1 (simulates 3x3 kernel at bottom position):\n",
            "  Input Channel 0 kernel: [0.0, 0.0, 0.0]\n",
            "  Input Channel 1 kernel: [0.10000000149011612, 0.20000000298023224, 0.30000001192092896]\n",
            "  Input Channel 2 kernel: [0.4000000059604645, 0.5, 0.6000000238418579]\n",
            "  Input Channel 3 kernel: [0.699999988079071, 0.800000011920929, 0.8999999761581421]\n",
            "1D Conv Output Shape: torch.Size([1, 2, 3])\n",
            "1D Conv Output (2 channels):\n",
            "tensor([[41.1000, 45.6000, 50.1000],\n",
            "        [63.6000, 68.1000, 72.6000]], grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note: you could in theory just use 1-d conv with varying channels for images, coloured images etc but then things get very complicated and you may not end up applying things as intended. So better stick with the intended methods and refer the papers for any nuanced implmentation."
      ],
      "metadata": {
        "id": "GJC7h6FejkO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A note on Conv Transpose"
      ],
      "metadata": {
        "id": "qErNTQR4k69d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its simple (its exactly as the name suggests)\n",
        "\n",
        "Let $y = Wx$ where W is the convolution matrix\n",
        "\n",
        "\n",
        "\n",
        "Then $W^T$ is the matrix which can be applied to the y\n",
        "\n",
        "$WW^T$ is a square matrix\n",
        "\n",
        "This is used for upscaling in encoder-decoder architecture (conv usually reduces the dimension)\n",
        "\n",
        "\n",
        "Note: its not an inverse operation, remember x and y are of not same dimension so its not possible mathematically\n",
        "\n",
        "\n",
        "Anyway in Cnns we dont share params across layers (they could go anywhere as per the training)"
      ],
      "metadata": {
        "id": "CGw3jaY7lckW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Shape calcuation, the roles flip:\n",
        "\n",
        "For example: with no dialation,stride\n",
        "\n",
        "Output_size = Input_size + Kernel_size - 1\n"
      ],
      "metadata": {
        "id": "9MKeGq1AoxDn"
      }
    }
  ]
}